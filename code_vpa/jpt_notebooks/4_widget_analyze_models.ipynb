{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code with interactive widgets to analyze trained models and plot validation and roc curves\n",
    "Sept 3, 2019\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps:\n",
    "- For a subset of models, read all data\n",
    "- Store it in a summary dictionary\n",
    "- Read from the dictionary for a specific model\n",
    "- Plot learning curve, roc curves and print summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "import subprocess as sp\n",
    "import pickle\n",
    "from ipywidgets import interact, interact_manual,fixed\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## M-L modules\n",
    "# import tensorflow.keras\n",
    "# from tensorflow.keras import layers, models, optimizers, callbacks  # or tensorflow.keras as keras\n",
    "# import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_get_data(filename):\n",
    "    '''\n",
    "    Function to get data from hdf5 files into images, labels and weights.\n",
    "    '''\n",
    "    try: \n",
    "        hf = h5py.File(filename)\n",
    "\n",
    "    except:\n",
    "        print(e)\n",
    "        print(\"Name of file\",filename)\n",
    "        raise SystemError\n",
    "\n",
    "    idx=50000\n",
    "    images = np.expand_dims(hf['all_events']['hist'][:idx], -1)\n",
    "    labels = hf['all_events']['y'][:idx]\n",
    "    weights = hf['all_events']['weight'][:idx]\n",
    "    weights = np.log(weights+1)\n",
    "\n",
    "    keys=['images','labels','weights']\n",
    "    values_dict=dict(zip(keys,[images,labels,weights]))\n",
    "\n",
    "    return values_dict\n",
    "\n",
    "\n",
    "def f_plot_learning(history):\n",
    "    '''Plot learning curves : Accuracy and Validation'''\n",
    "    fig=plt.figure()\n",
    "    # Plot training & validation accuracy values\n",
    "    fig.add_subplot(2,1,1)\n",
    "    xlim=len(history['acc'])\n",
    "    \n",
    "    plt.plot(history['acc'],label='Train',marker='o')\n",
    "    plt.plot(history['val_acc'],label='Validation',marker='*')\n",
    "#     plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xticks(np.arange(0,xlim,2))\n",
    "    \n",
    "    # Plot loss values\n",
    "    fig.add_subplot(2,1,2)\n",
    "    plt.plot(history['loss'],label='Train',marker='o')\n",
    "    plt.plot(history['val_loss'],label='Validation',marker='*')\n",
    "#     plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.xticks(np.arange(0,xlim,2))\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "\n",
    "\n",
    "def f_plot_roc_curve(fpr,tpr):\n",
    "    '''\n",
    "    Module for roc plot and printing AUC\n",
    "    '''\n",
    "    plt.figure()\n",
    "    # plt.plot(fpr,tpr)\n",
    "    plt.scatter(fpr,tpr)\n",
    "    plt.semilogx(fpr, tpr)\n",
    "  # Zooms\n",
    "    plt.xlim([10**-7,1.0])\n",
    "    plt.ylim([0,1.0])\n",
    "    # y=x line for comparison\n",
    "    x=np.linspace(0,1,num=500)\n",
    "    plt.plot(x,x)\n",
    "#     plt.xscale('log')\n",
    "#     plt.xlim(1e-10,1e-5)\n",
    "\n",
    "    # AUC \n",
    "#     auc_val = auc(fpr, tpr)\n",
    "#     print(\"AUC: \",auc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read stored model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_read_stored_data(model_save_dir,model_name):\n",
    "    '''\n",
    "    Read model, history and predictions\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    fname_model='model_{0}.h5'.format(model_name)\n",
    "    fname_history='history_{0}.pickle'.format(model_name)\n",
    "\n",
    "    # Load model and history\n",
    "    model=load_model(model_save_dir+fname_model)\n",
    "    with open(model_save_dir+fname_history,'rb') as f:\n",
    "        history= pickle.load(f)\n",
    "\n",
    "    # Load predictions\n",
    "    # y_pred=model.predict(test_x,verbose=1)\n",
    "    fname_ypred=model_save_dir+'ypred_{0}.test'.format(model_name)\n",
    "#     print(fname_ypred)\n",
    "    y_pred=np.loadtxt(fname_ypred)\n",
    "    \n",
    "    \n",
    "    return model, history, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Since reading data takes a bit of time, we first read a subset of models and then analyze them\n",
    "\n",
    "def f_real_all_data(model_save_dir,model_name_list):\n",
    "    '''\n",
    "    Read stored data, plot learning and roc curves, print model summary\n",
    "    '''\n",
    "    \n",
    "    dict_summary=dict.fromkeys(model_name_list,None)\n",
    "    \n",
    "    for model_name in model_name_list:\n",
    "#         model_save_dir='/global/project/projectdirs/dasrepo/vpa/atlas_cnn/results/'\n",
    "        model,history,y_pred=f_read_stored_data(model_save_dir,model_name)\n",
    "\n",
    "        ### Extract the training and validation data\n",
    "        data_dir='/global/project/projectdirs/dasrepo/vpa/atlas_cnn/data/RPVSusyData/'\n",
    "\n",
    "        #### Test_data\n",
    "        filename=data_dir+'val.h5'\n",
    "        test_data_dict=f_get_data(filename)\n",
    "        \n",
    "        dict1={'name':model_name,'model':model,'history':history,'y_pred':y_pred, 'test_data':test_data_dict}\n",
    "        dict_summary[model_name]=dict1\n",
    "        \n",
    "    return dict_summary\n",
    "\n",
    "\n",
    "def f_analyze_model(model_name,dict_summary,learning_curve=True,plot_roc=True,summary=False):\n",
    "    '''\n",
    "    Analyze model\n",
    "    '''\n",
    "\n",
    "    ### Pick up data stored in summary dictionary\n",
    "    dict1=dict_summary[model_name]\n",
    "    model,history,test_data_dict,y_pred=dict1['model'],dict1['history'],dict1['test_data'],dict1['y_pred']\n",
    "    \n",
    "    \n",
    "    test_x,test_y,test_wts=test_data_dict['images'],test_data_dict['labels'],test_data_dict['weights']\n",
    "#     print(test_x.shape,test_y.shape,test_wts.shape)\n",
    "\n",
    "    ## roc curve\n",
    "    fpr,tpr,threshold=roc_curve(test_y,y_pred,sample_weight=test_wts)\n",
    "#     print(fpr.shape,tpr.shape,threshold.shape)\n",
    "    # Plot tested model\n",
    "    if learning_curve: f_plot_learning(history)\n",
    "        \n",
    "    ## Plot roc curve\n",
    "    if plot_roc: f_plot_roc_curve(fpr,tpr)\n",
    "    \n",
    "    ## Model summary\n",
    "    if summary: model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First store data for a subset of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir='/global/project/projectdirs/dasrepo/vpa/atlas_cnn/results/1_test_Sept4_2019_50k_pts/'\n",
    "model_list=['1','2','3','4','5','20','30']\n",
    "model_sublist=['1','2','3','4','5','20','30']\n",
    "dict_summary=f_real_all_data(model_save_dir,model_sublist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate plots and summary \n",
    "Read from dictionary **dict_summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_analyze_model('1',dict_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_manual(f_analyze_model,dict_summary=fixed(dict_summary),model_name=model_sublist )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v_py3",
   "language": "python",
   "name": "v_jpt_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
